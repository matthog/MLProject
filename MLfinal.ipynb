{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmP74cigyJnnK9OD9Fbd/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthog/MLProject/blob/main/MLfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "# Load data\n",
        "url = 'https://raw.githubusercontent.com/matthog/MLHW/refs/heads/main/output.json'\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "# Prepare data\n",
        "locations = []\n",
        "square_footage = []\n",
        "property_values = []\n",
        "target_sales = []\n",
        "item_totals = []\n",
        "time_of_year = []\n",
        "\n",
        "for key, entry in data.items():\n",
        "    locations.append([entry['coordinates']['latitude'], entry['coordinates']['longitude']])\n",
        "    square_footage.append(entry['square_footage'])\n",
        "    property_values.append(entry['property_value'])\n",
        "    target_sales.append(entry['csv_data']['total_sales'])\n",
        "    item_totals.append(entry['csv_data']['total_items_sold'])\n",
        "    time_of_year.append(entry['sale_date'])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "locations = np.array(locations)\n",
        "square_footage = np.array(square_footage).reshape(-1, 1)\n",
        "property_values = np.array(property_values).reshape(-1, 1)\n",
        "time_of_year = np.array(time_of_year)\n",
        "target_sales = np.array(target_sales)\n",
        "item_totals = np.array(item_totals)\n",
        "\n",
        "# Cyclical encoding for time of year\n",
        "# Map month names to numbers\n",
        "month_to_num = {\n",
        "    \"January\": 1, \"February\": 2, \"March\": 3, \"April\": 4,\n",
        "    \"May\": 5, \"June\": 6, \"July\": 7, \"August\": 8,\n",
        "    \"September\": 9, \"October\": 10, \"November\": 11, \"December\": 12\n",
        "}\n",
        "\n",
        "# Extract and map the sale_date\n",
        "time_of_year = [month_to_num[entry['sale_date']] for key, entry in data.items()]\n",
        "\n",
        "#Convert time_of_year to a numpy array\n",
        "time_of_year = np.array(time_of_year, dtype=float)\n",
        "\n",
        "# cyclical encoding for time of year (month)\n",
        "time_of_year_sin = np.sin(2 * np.pi * time_of_year / 12)\n",
        "time_of_year_cos = np.cos(2 * np.pi * time_of_year / 12)\n",
        "time_of_year_encoded = np.column_stack((time_of_year_sin, time_of_year_cos))\n",
        "\n",
        "# Normalize features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "square_footage = scaler.fit_transform(square_footage)\n",
        "property_values = scaler.fit_transform(property_values)\n",
        "locations = scaler.fit_transform(locations)\n",
        "\n",
        "# Combine all features into a single input array\n",
        "features = np.hstack([square_footage, property_values, locations, time_of_year_encoded])\n",
        "\n",
        "# Scale target_sales and item_totals\n",
        "sales_scaler = MinMaxScaler()\n",
        "items_scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the target values\n",
        "target_sales_scaled = sales_scaler.fit_transform(target_sales.reshape(-1, 1))\n",
        "item_totals_scaled = items_scaler.fit_transform(item_totals.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train_sales, y_test_sales = train_test_split(\n",
        "    features, target_sales_scaled, test_size=0.1, random_state=42\n",
        ")\n",
        "_, _, y_train_items, y_test_items = train_test_split(\n",
        "    features, item_totals_scaled, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Build the neural network model\n",
        "def build_model(input_dim):\n",
        "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
        "    dense1 = tf.keras.layers.Dense(128, activation='relu')(input_layer)\n",
        "    batch_norm1 = tf.keras.layers.BatchNormalization()(dense1)\n",
        "    dense2 = tf.keras.layers.Dense(128, activation='relu')(batch_norm1)\n",
        "    batch_norm2 = tf.keras.layers.BatchNormalization()(dense2)\n",
        "    dropout = tf.keras.layers.Dropout(0.3)(batch_norm2)\n",
        "\n",
        "    output_sales = tf.keras.layers.Dense(1, name='sales_output')(dropout)\n",
        "    output_items = tf.keras.layers.Dense(1, name='items_output')(dropout)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=[output_sales, output_items])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "        loss={'sales_output': 'Huber', 'items_output': 'Huber'},\n",
        "        metrics={'sales_output': ['mae'], 'items_output': ['mae']}\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model(features.shape[1])\n",
        "\n",
        "# Train the model with callbacks\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.9, patience=10, min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Train the model without early stopping\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    {'sales_output': y_train_sales, 'items_output': y_train_items},\n",
        "    validation_data=(X_test, {'sales_output': y_test_sales, 'items_output': y_test_items}),\n",
        "    epochs=50,  # Train for much longer if necessary\n",
        "    batch_size=32,\n",
        "    callbacks=[reduce_lr],  # Only reduce_lr callback\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation = model.evaluate(X_test, {'sales_output': y_test_sales, 'items_output': y_test_items})\n",
        "print(\"\\nEvaluation Results:\", evaluation)\n",
        "\n",
        "# Predictions\n",
        "pred_sales, pred_items = model.predict(X_test)\n",
        "\n",
        "# Reverse the scaling of predictions\n",
        "pred_sales_unscaled = sales_scaler.inverse_transform(pred_sales)\n",
        "pred_items_unscaled = items_scaler.inverse_transform(pred_items)\n",
        "\n",
        "# Calculate RMSE for better interpretability\n",
        "rmse_sales = np.sqrt(mean_squared_error(y_test_sales, pred_sales_unscaled))\n",
        "rmse_items = np.sqrt(mean_squared_error(y_test_items, pred_items_unscaled))\n",
        "print(f\"\\nRMSE for Sales: {rmse_sales}\")\n",
        "print(f\"RMSE for Items Sold: {rmse_items}\")\n",
        "\n",
        "# Display actual vs predicted values\n",
        "print(\"\\nActual vs Predicted Sales:\")\n",
        "for actual, predicted in zip(sales_scaler.inverse_transform(y_test_sales)[:10].flatten(), pred_sales_unscaled.flatten()[:10]):\n",
        "    print(f\"Actual: {actual:.2f}, Predicted: {predicted:.2f}\")\n",
        "\n",
        "print(\"\\nActual vs Predicted Items:\")\n",
        "for actual, predicted in zip(items_scaler.inverse_transform(y_test_items)[:10].flatten(), pred_items_unscaled.flatten()[:10]):\n",
        "    print(f\"Actual: {actual:.2f}, Predicted: {predicted:.2f}\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XI-ZpFy5VtW1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}